
# base config
langs: [en, de]
epoch: 150
update_every_epoch: 1000
dumpdir: ./data/exp/paper/wmtende

share_all_task_model: True
optimizer: adam 
learning_rate: 0.0007
learning_rate_warmup: 4000
scheduling: warmupexponentialdecay
max_tokens: 16384
max_seq_length: 512
save_interval: 1
weight_decay: 0
adam_betas: [0.9, 0.98]
log_interval: 100

clip_grad_norm: 0
label_smoothing: 0.1
accumulate_gradients: 1
share_all_embedd: True
share_out_embedd: True
patience: 10
keep_last_checkpoint: 20

tasks:
  wmtende_mt:
    task_name: seq2seq
    reload_checkpoint:
    data:
        data_folder:  ./data/train_data/wmt16_ende/bin
        src_vocab: vocab.bpe.32000
        tgt_vocab: vocab.bpe.32000
        train_valid_test: [train.bpe.en.pth:train.bpe.de.pth, valid.bpe.en.pth:valid.bpe.de.pth, valid.bpe.en.pth:valid.bpe.de.pth]
        group_by_size: True
        split_data: 15
        max_len: 200

    sentenceRep:
      type: transformer 
      hidden_size: 512
      ff_size: 2048
      attention_dropout: 0.1
      encoder_layers: 6
      num_heads: 8
      embedd_size: 512
      dropout: 0.1
      learned_pos: True
      activation: relu

    target:
      type: transformer 
      hidden_size: 512
      ff_size: 2048
      attention_dropout: 0.1
      decoder_layers: 6
      num_heads: 8
      embedd_size: 512
      dropout: 0.1
      learned_pos: True
      activation: relu
